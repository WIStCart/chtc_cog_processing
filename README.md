# SCO GeoTiff Workflow Tool

## Overview

This workflow tool can be used to process geotiff images for the SCO's Whaifinder image
generation pipeline.

The tool has three main components:
- A periodic monitor (HTCondor Cron job, or Crondor for short) that watches for triggers indicating
there is work to do
- A Directed Acyclic Graph (DAG) workflow that runs any jobs generated by the trigger
- A post-processing script that manages state tracking for successful/failed jobs run through the DAG


When the script is first run, a Crondor job is submitted on the AP. This Crondor is responsible for checking
an input S3 bucket every 15 minutes for CSV files that match `*raw.csv`, where the CSV is assumed to contain
all of the information necessary for the underlying `generate_cogs.sh` executable to run. Importantly, the
appearance of a matching file **MUST** indicate that _all_ files detailed in the CSV are already present in the
input bucket. In other words, don't upload the CSV until everything else is finished uploading.

When a matching CSV is found, the Crondor downloads the file to the AP and uses it to generate and submit a
DAG of conversion work. After the DAG is submitted, the script renames the remote `*raw.csv` files to something
like `*processing-<TIMESTAMP>.csv` to indicate they are currently being processed. This is needed to avoid
re-processing the same inputs.

Each node/job in the DAG is responsible for the conversion of one image. After the image is converted, the
node/job moves the outputs (one .tif and one .jpg) to the output bucket.

When all nodes/jobs in the DAG have completed, the post-processing script checks the output bucket for all of
expected files. If any image pairs are missing, the script writes the corresponding row from the `*raw.csv`
to a file that aggregates conversion failures. Likewise, any image pairs that are found in the output bucket
are written to a file that aggregates success. These two files the replace the `processing-<TIMESTAMP>` file
in the input bucket and can be used for monitoring jobs that might require intervention.


## Submitting a workflow
Start by cloning this repository to the AP and changing to its root with `cd sco_crank`.

In order to run this script, you'll need to set up several environment pieces. After installing conda (use the
"Create a Miniconda Installation" instructions [here](https://chtc.cs.wisc.edu/uw-research-computing/conda-installation)),
create the required conda environment by running `conda env create -f environment.yml`. After the environment is created,
run `conda activate sco_crank`. Your environment should be set up now.

This script assumes access to an input and output bucket (which may be the same bucket) that are hosted at some S3 endpoint. 
By default, we assume the CHTC S3 instance hosted at s3dev.chtc.wisc.edu, although any endpoint can be used as long as you
possess a valid access/secret key pair.

To launch the Crondor job that triggers workflow, run:
```
python3 crank.py  -a <ACCESS KEY FILE PATH> -s <SECRET KEY FILE PATH> -e <S3 ENDPOINT URL> --input-bucket <INPUT BUCKET> --output-bucket <OUTPUT BUCKET> -p "*raw.csv"
```

When matching files are found, a new directory is created wherever the script was initially run. This directory will be something
like `workflow-run-<TIMESTAMP>`. All files Condor creates as a part of the DAG, as well as the CSV files of images, will be
located here.

**NOTE:** If any jobs are placed on hold with an error that indicates job credentials aren't available, try running
`condor_submit scitokens_workaround.sub` before launching the DAG. This submits a hold job that forces Condor
to make fresh credentials available to the DAG.